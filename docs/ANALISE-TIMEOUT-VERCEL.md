# ğŸ” AnÃ¡lise TÃ©cnica: Timeout do Vercel (300 segundos)

**Data:** 30 de dezembro de 2025  
**Issue:** #1  
**Criticidade:** ğŸ”¥ CRÃTICA  
**Status:** EM ANÃLISE

---

## ğŸ“‹ Contexto

**Problema relatado:**

```
Vercel Runtime Timeout Error: Task timed out after 300 seconds
```

**Quando ocorre:**

- Legendas com >500 linhas
- Processamento sequencial demora >5 minutos
- AplicaÃ§Ã£o trava e nÃ£o retorna resposta

**Por que funciona no Render:**

- Render **NÃƒO tem timeout de execuÃ§Ã£o** (ou tem limite muito maior, tipo 30-60 minutos)
- Render usa instÃ¢ncias persistentes (nÃ£o serverless)
- Vercel usa **serverless functions** com limite rÃ­gido de 300s

---

## ğŸ—ï¸ Arquitetura Atual

### Como estÃ¡ funcionando AGORA:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Cliente  â”‚â”€â”€â”€â”€POST /apiâ”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Vercel Edge     â”‚
â”‚  (Browser) â”‚                      â”‚  (Serverless)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                                      â”‚
      â”‚                                      â”‚
      â”‚                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚                            â”‚  route.ts         â”‚
      â”‚                            â”‚  (851 linhas)     â”‚
      â”‚                            â”‚                   â”‚
      â”‚                            â”‚  Loop sequencial: â”‚
      â”‚                            â”‚  for (chunk of    â”‚
      â”‚                            â”‚       chunks) {   â”‚
      â”‚                            â”‚    await gemini() â”‚
      â”‚                            â”‚  }                â”‚
      â”‚                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                                      â”‚
      â”‚â—€â”€â”€â”€â”€SSE Stream Progressâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
      â”‚     (data: {...})                    â”‚
      â”‚                                      â”‚
      â”‚                            â±ï¸ TIMEOUT depois de
      â”‚                               300 segundos!
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€âŒ ERRO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Por que estÃ¡ quebrando:

1. **UMA Ãºnica requisiÃ§Ã£o HTTP**
   - Cliente faz POST â†’ Servidor processa TUDO â†’ Retorna resultado
   - Se demora >300s = timeout

2. **Processamento bloqueante**
   - `for await` loop esperando cada chunk
   - Cada chunk demora ~3-5 segundos
   - 100 chunks Ã— 5s = 500 segundos âŒ

3. **Serverless nÃ£o Ã© para long-running tasks**
   - Vercel limita funÃ§Ãµes em 10s (hobby), 60s (Pro), 300s (mÃ¡ximo com configuraÃ§Ã£o)
   - Mesmo com Pro, nÃ£o resolve para arquivos grandes

---

## ğŸ¯ Todas as SoluÃ§Ãµes PossÃ­veis

Vou analisar TODAS as opÃ§Ãµes, nÃ£o sÃ³ as 3 mencionadas no ROADMAP.

---

### âœ… OpÃ§Ã£o 1: Streaming com Chunks Menores (MAIS SIMPLES)

**Arquitetura:**

```
Cliente                      Servidor (Vercel)
   â”‚                              â”‚
   â”‚â”€â”€â”€â”€POST /api/translateâ”€â”€â”€â”€â”€â”€â–¶â”‚
   â”‚  chunk1: [linhas 1-50]       â”‚
   â”‚                              â”‚â”€â”€â”€â”€ gemini() â”€â”€â”€â”€â–¶ Gemini
   â”‚â—€â”€â”€â”€â”€response chunk1â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â—€â”€â”€â”€â”€ traduÃ§Ã£o â”€â”€â”€â”˜
   â”‚                              â”‚
   â”‚â”€â”€â”€â”€POST /api/translateâ”€â”€â”€â”€â”€â”€â–¶â”‚
   â”‚  chunk2: [linhas 51-100]     â”‚
   â”‚                              â”‚â”€â”€â”€â”€ gemini() â”€â”€â”€â”€â–¶ Gemini
   â”‚â—€â”€â”€â”€â”€response chunk2â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â—€â”€â”€â”€â”€ traduÃ§Ã£o â”€â”€â”€â”˜
   â”‚                              â”‚
   â”‚  (repete atÃ© terminar)       â”‚
```

**Como funciona:**

- Cliente divide arquivo em **chunks pequenos** (~30-50 linhas)
- Faz **mÃºltiplas requisiÃ§Ãµes HTTP** sequenciais
- Cada requisiÃ§Ã£o processa 1 chunk e retorna em <30s
- Cliente concatena os resultados no final

**ImplementaÃ§Ã£o:**

**Cliente (React):**

```typescript
async function translateFile(file: File, apiKey: string) {
	const content = await file.text();
	const segments = parseSegmentsFromSRT(content);

	// Dividir em chunks pequenos (30-50 linhas cada)
	const LINES_PER_CHUNK = 40;
	const chunks = [];

	for (let i = 0; i < segments.length; i += LINES_PER_CHUNK) {
		chunks.push(segments.slice(i, i + LINES_PER_CHUNK));
	}

	const translatedSegments: Segment[] = [];

	for (let i = 0; i < chunks.length; i++) {
		const chunk = chunks[i];

		// Fazer POST para cada chunk
		const response = await fetch('/api/translate', {
			method: 'POST',
			headers: { 'Content-Type': 'application/json' },
			body: JSON.stringify({
				segments: chunk,
				apiKey,
				chunkIndex: i,
				totalChunks: chunks.length,
			}),
		});

		const result = await response.json();
		translatedSegments.push(...result.translatedSegments);

		// Atualizar progresso
		setProgress({
			current: translatedSegments.length,
			total: segments.length,
			percentage: (translatedSegments.length / segments.length) * 100,
		});
	}

	return buildSRTFromSegments(translatedSegments);
}
```

**Servidor (route.ts):**

```typescript
export async function POST(request: Request) {
	const { segments, apiKey, chunkIndex, totalChunks } = await request.json();

	// Processar apenas ESTE chunk (nÃ£o todos)
	const textToTranslate = segments.map((s) => s.text).join('|');

	// Traduzir (rÃ¡pido, <30s)
	const translated = await translateWithGemini(textToTranslate, apiKey);

	// Retornar resultado IMEDIATAMENTE
	return Response.json({
		translatedSegments: parseTranslatedSegments(translated, segments),
		chunkIndex,
		totalChunks,
	});
}
```

**PrÃ³s:**

- âœ… **Mais simples** de implementar (~4-6 horas)
- âœ… Funciona no Vercel sem mudanÃ§as de infra
- âœ… Cada requisiÃ§Ã£o <30s = sem timeout
- âœ… Cliente tem controle total (pode pausar, cancelar, retomar)
- âœ… Pode processar arquivos ENORMES (10.000+ linhas)
- âœ… Progresso em tempo real natural (nÃ£o precisa polling)

**Contras:**

- âŒ Mais requisiÃ§Ãµes HTTP (impacto mÃ­nimo, Ã© rÃ¡pido)
- âŒ Cliente precisa gerenciar estado (mas jÃ¡ faz isso com chunks)
- âŒ NÃ£o resolve rate limiting (mas isso Ã© problema separado)

**PrincÃ­pios de Arquitetura:**

- âœ… **SRP:** Cada requisiÃ§Ã£o = 1 chunk
- âœ… **Stateless:** Servidor nÃ£o guarda estado entre requisiÃ§Ãµes
- âœ… **EscalÃ¡vel:** NÃ£o depende de backend persistente
- âœ… **Resiliente:** Se uma requisiÃ§Ã£o falha, sÃ³ reprocessa aquele chunk

**Estimativa:** 4-6 horas de implementaÃ§Ã£o

---

### âœ… OpÃ§Ã£o 2: Background Jobs com Polling (MAIS PROFISSIONAL)

**Arquitetura:**

```
Cliente                   API Gateway              Worker           Storage
   â”‚                          â”‚                      â”‚                â”‚
   â”‚â”€POST /api/translateâ”€â”€â”€â”€â”€â–¶â”‚                      â”‚                â”‚
   â”‚  {file, apiKey}           â”‚â”€â”€create jobâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚
   â”‚                           â”‚                      â”‚    Redis/DB    â”‚
   â”‚â—€â”€â”€{jobId: "abc123"}â”€â”€â”€â”€â”€â”€â”€â”‚                      â”‚    jobs table  â”‚
   â”‚                           â”‚                      â”‚                â”‚
   â”‚                           â”‚                      â”‚â—€â”€â”€â”€â”€jobâ”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚                           â”‚                      â”‚  status:pendingâ”‚
   â”‚                           â”‚                      â”‚                â”‚
   â”‚                           â”‚                      â”‚â”€â”€processâ”€â”€â”€â”€â”€â”€â”€â–¶
   â”‚                           â”‚                      â”‚   chunks
   â”‚                           â”‚                      â”‚                â”‚
   â”‚                           â”‚                      â”‚â”€â”€updateâ”€â”€â”€â”€â”€â”€â”€â”€â–¶
   â”‚                           â”‚                      â”‚  progress 30%  â”‚
   â”‚                           â”‚                      â”‚                â”‚
   â”‚â”€GET /api/status/abc123â”€â”€â”€â–¶â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚
   â”‚â—€â”€â”€{progress: 30%}â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚                           â”‚                      â”‚                â”‚
   â”‚  (polling a cada 2s)      â”‚                      â”‚                â”‚
   â”‚                           â”‚                      â”‚â”€â”€updateâ”€â”€â”€â”€â”€â”€â”€â”€â–¶
   â”‚                           â”‚                      â”‚  status:done   â”‚
   â”‚                           â”‚                      â”‚  result: {...} â”‚
   â”‚â”€GET /api/status/abc123â”€â”€â”€â–¶â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚
   â”‚â—€â”€â”€{status: 'done', â”€â”€â”€â”€â”€â”€â”€â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚    result: "..."}          â”‚                      â”‚                â”‚
```

**Como funciona:**

1. Cliente faz POST inicial â†’ retorna `jobId` imediatamente
2. Servidor cria job no Redis/DB com status "pending"
3. Worker assÃ­ncrono processa o job no background
4. Cliente faz polling GET `/api/status/{jobId}` a cada 2-3 segundos
5. Quando completo, cliente baixa o resultado

**ImplementaÃ§Ã£o:**

**Storage (Redis/Upstash):**

```typescript
interface Job {
	id: string;
	status: 'pending' | 'processing' | 'done' | 'error';
	progress: number;
	totalSegments: number;
	translatedSegments: number;
	result?: string;
	error?: string;
	createdAt: number;
	updatedAt: number;
}

// jobs:{jobId} = JSON.stringify(job)
```

**API POST /api/translate:**

```typescript
export async function POST(request: Request) {
	const { content, apiKey, filename } = await request.json();

	const jobId = generateUniqueId(); // uuid v4

	// Salvar job no Redis
	await redis.set(
		`jobs:${jobId}`,
		JSON.stringify({
			id: jobId,
			status: 'pending',
			progress: 0,
			totalSegments: parseSegments(content).length,
			translatedSegments: 0,
			createdAt: Date.now(),
			updatedAt: Date.now(),
		}),
		{ ex: 3600 },
	); // TTL 1 hora

	// Salvar input no Redis (temporÃ¡rio)
	await redis.set(
		`jobs:${jobId}:input`,
		JSON.stringify({
			content,
			apiKey,
			filename,
		}),
		{ ex: 3600 },
	);

	// Retornar jobId IMEDIATAMENTE (sem esperar)
	return Response.json({ jobId });
}
```

**Worker (Vercel Cron ou separado):**

```typescript
// Pode ser:
// 1. Vercel Cron (/api/cron/process-jobs) que roda a cada 10s
// 2. Vercel Edge Function com timeout maior
// 3. Separate worker (Railway, Render, AWS Lambda)

export async function processJob(jobId: string) {
	// Pegar input
	const input = await redis.get(`jobs:${jobId}:input`);
	const { content, apiKey, filename } = JSON.parse(input);

	// Atualizar status
	await updateJob(jobId, { status: 'processing' });

	try {
		const segments = parseSegments(content);
		const chunks = groupIntoChunks(segments);
		const translatedSegments = [];

		for (let i = 0; i < chunks.length; i++) {
			const chunk = chunks[i];

			// Traduzir chunk
			const translated = await translateWithGemini(chunk, apiKey);
			translatedSegments.push(...translated);

			// Atualizar progresso
			await updateJob(jobId, {
				progress: Math.round(
					(translatedSegments.length / segments.length) * 100,
				),
				translatedSegments: translatedSegments.length,
				updatedAt: Date.now(),
			});
		}

		// Finalizar
		const result = buildSRT(translatedSegments);
		await updateJob(jobId, {
			status: 'done',
			result,
			progress: 100,
			updatedAt: Date.now(),
		});
	} catch (error) {
		await updateJob(jobId, {
			status: 'error',
			error: error.message,
			updatedAt: Date.now(),
		});
	} finally {
		// Limpar input (nÃ£o precisa mais)
		await redis.del(`jobs:${jobId}:input`);
	}
}
```

**API GET /api/status/:jobId:**

```typescript
export async function GET(
	request: Request,
	{ params }: { params: { jobId: string } },
) {
	const { jobId } = params;

	const job = await redis.get(`jobs:${jobId}`);

	if (!job) {
		return Response.json({ error: 'Job not found' }, { status: 404 });
	}

	return Response.json(JSON.parse(job));
}
```

**Cliente (React com polling):**

```typescript
async function translateFile(file: File, apiKey: string) {
	// 1. Iniciar job
	const { jobId } = await fetch('/api/translate', {
		method: 'POST',
		body: JSON.stringify({ content: await file.text(), apiKey }),
	}).then((r) => r.json());

	// 2. Polling a cada 2 segundos
	return new Promise((resolve, reject) => {
		const interval = setInterval(async () => {
			const job = await fetch(`/api/status/${jobId}`).then((r) => r.json());

			// Atualizar UI
			setProgress({
				percentage: job.progress,
				current: job.translatedSegments,
				total: job.totalSegments,
			});

			if (job.status === 'done') {
				clearInterval(interval);
				resolve(job.result);
			}

			if (job.status === 'error') {
				clearInterval(interval);
				reject(new Error(job.error));
			}
		}, 2000);
	});
}
```

**PrÃ³s:**

- âœ… **Sem timeout** (worker roda quanto tempo precisar)
- âœ… **EscalÃ¡vel** (pode ter mÃºltiplos workers)
- âœ… **Resiliente** (se worker crashar, outro pode continuar)
- âœ… **Profissional** (padrÃ£o da indÃºstria)
- âœ… **Pode processar arquivos ENORMES** (10.000+ linhas)
- âœ… **Cliente pode fechar browser** e voltar depois (com jobId)

**Contras:**

- âŒ **Mais complexo** (~2-3 dias de implementaÃ§Ã£o)
- âŒ **Requer persistÃªncia** (Redis/Upstash ou DB)
- âŒ **Custo adicional** (Upstash Redis ou PostgreSQL)
- âŒ **Mais moving parts** (mais coisas que podem quebrar)
- âŒ **LatÃªncia inicial** (precisa criar job antes de comeÃ§ar)

**PrincÃ­pios de Arquitetura:**

- âœ… **Separation of Concerns:** API â‰  Worker
- âœ… **Async Processing:** NÃ£o bloqueia requisiÃ§Ã£o
- âœ… **Stateful:** Usa storage para compartilhar estado
- âœ… **Scalable:** Workers podem escalar horizontalmente
- âœ… **Observable:** Progresso rastreÃ¡vel via polling

**Estimativa:** 2-3 dias de implementaÃ§Ã£o

---

### âœ… OpÃ§Ã£o 3: WebSockets Real-Time (OVER-ENGINEERED)

**Arquitetura:**

```
Cliente                   WebSocket Server         Worker
   â”‚                            â”‚                     â”‚
   â”‚â”€â”€â”€â”€â”€connect ws://â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚                     â”‚
   â”‚                             â”‚                     â”‚
   â”‚â”€â”€msg: {type: 'start'}â”€â”€â”€â”€â”€â”€â–¶â”‚â”€â”€â”€â”€â”€dispatchâ”€â”€â”€â”€â”€â”€â”€â–¶â”‚
   â”‚                             â”‚                     â”‚
   â”‚â—€â”€â”€msg: {type: 'progress'}â”€â”€â”€â”‚â—€â”€â”€â”€â”€updateâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚    {progress: 20%}           â”‚                     â”‚
   â”‚                             â”‚                     â”‚
   â”‚â—€â”€â”€msg: {type: 'progress'}â”€â”€â”€â”‚â—€â”€â”€â”€â”€updateâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚    {progress: 40%}           â”‚                     â”‚
   â”‚                             â”‚                     â”‚
   â”‚â—€â”€â”€msg: {type: 'done'}â”€â”€â”€â”€â”€â”€â”€â”‚â—€â”€â”€â”€â”€doneâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚    {result: "..."}           â”‚                     â”‚
```

**Como funciona:**

- Cliente abre **conexÃ£o WebSocket**
- Servidor processa chunks e envia **updates em tempo real** via WS
- Sem polling (push-based)

**PrÃ³s:**

- âœ… **Real-time** (sem delay de polling)
- âœ… **Eficiente** (nÃ£o faz vÃ¡rias requisiÃ§Ãµes HTTP)
- âœ… **Modern** (padrÃ£o para apps real-time)

**Contras:**

- âŒ **Muito complexo** (~4-5 dias)
- âŒ **Vercel nÃ£o suporta WS** nativamente (precisa de outro servidor)
- âŒ **Over-engineering** para este caso de uso
- âŒ **Mais caro** (precisa servidor persistente para WS)

**Estimativa:** 4-5 dias + infra adicional

**NÃ£o recomendo** para este projeto.

---

### âœ… OpÃ§Ã£o 4: Server-Sent Events (SSE) - JÃ ESTÃ USANDO!

Espera... **vocÃªs JÃ ESTÃƒO usando SSE**! ğŸ¤”

Olhando o cÃ³digo atual:

```typescript
// app/api/route.ts
export async function POST(request: Request) {
	const encoder = new TextEncoder();

	const stream = new ReadableStream({
		async start(controller) {
			// ...processar chunks...

			// Enviar progresso via SSE
			const progressData = {
				type: 'progress',
				translated: translatedSegments.length,
				total: segments.length,
				percentage: Math.round(
					(translatedSegments.length / segments.length) * 100,
				),
			};
			controller.enqueue(
				encoder.encode(`data: ${JSON.stringify(progressData)}\n\n`),
			);
		},
	});

	return new Response(stream, {
		headers: {
			'Content-Type': 'text/event-stream',
			'Cache-Control': 'no-cache',
			Connection: 'keep-alive',
		},
	});
}
```

**O problema NÃƒO Ã© a arquitetura!**  
**O problema Ã© processar TUDO em UMA Ãºnica requisiÃ§Ã£o!**

**A soluÃ§Ã£o Ã© simples:** Manter SSE, mas fazer mÃºltiplas requisiÃ§Ãµes curtas.

---

### âœ… OpÃ§Ã£o 5: Migrar para Render/Railway (NÃƒO RESOLVE O PROBLEMA)

**Por que funciona no Render:**

- Render usa **instÃ¢ncias persistentes** (nÃ£o serverless)
- Sem limite de 300s
- Pode rodar por horas

**PrÃ³s:**

- âœ… **Resolve imediatamente** (2 horas)
- âœ… **Sem cÃ³digo novo**

**Contras:**

- âŒ **NÃ£o resolve o problema raiz** (cÃ³digo ainda ruim)
- âŒ **Perde benefÃ­cios do Vercel** (Edge, CDN global, preview deployments)
- âŒ **Mais caro** (Render cobra por instÃ¢ncia sempre ligada)
- âŒ **NÃ£o escala bem** (instÃ¢ncia tem limite de RAM/CPU)

**NÃ£o recomendo** como soluÃ§Ã£o definitiva.

---

### âœ… OpÃ§Ã£o 6: Aumentar Timeout do Vercel (NÃƒO FUNCIONA)

**Limites do Vercel:**

- Hobby: 10s
- Pro: 60s (padrÃ£o), atÃ© 300s (config)
- Enterprise: atÃ© 900s (15 minutos)

**Mas:**

- âŒ VocÃª JÃ estÃ¡ no mÃ¡ximo (300s)
- âŒ Arquivos grandes ainda vÃ£o estourar
- âŒ NÃ£o resolve o problema de design

**NÃ£o Ã© soluÃ§Ã£o.**

---

## ğŸ† RecomendaÃ§Ã£o Final

### Para AGORA (resolver urgente): **OpÃ§Ã£o 1 - MÃºltiplas RequisiÃ§Ãµes**

**Por quÃª:**

1. **Mais simples** (~4-6 horas vs 2-3 dias)
2. **Funciona no Vercel** (sem infra adicional)
3. **Resolve o timeout** completamente
4. **MantÃ©m SSE** para progresso
5. **EscalÃ¡vel** (processa arquivos enormes)
6. **PrincÃ­pios de arquitetura** sÃ³lidos (stateless, SRP)

**Como implementar:**

1. Modificar cliente para dividir arquivo em chunks pequenos (30-50 linhas)
2. Fazer loop de requisiÃ§Ãµes POST sequenciais
3. Cada POST processa 1 chunk e retorna JSON (nÃ£o SSE!)
4. Cliente concatena resultados e atualiza barra de progresso
5. No final, monta SRT completo

**Tempo:** 4-6 horas

---

### Para FUTURO (quando monetizar): **OpÃ§Ã£o 2 - Background Jobs**

**Quando migrar:**

- Quando implementar autenticaÃ§Ã£o/pagamentos
- Quando tiver >1000 usuÃ¡rios
- Quando precisar de features como:
  - "Sair do site e voltar depois"
  - "Processar vÃ¡rios arquivos em paralelo"
  - "Fila de prioridade (usuÃ¡rios pagos primeiro)"

**O que precisa:**

- Redis (Upstash: ~$10/mÃªs)
- Worker separado (Vercel Cron ou Railway)
- Sistema de jobs com status

---

## ğŸš€ Plano de AÃ§Ã£o Recomendado

### SPRINT 1 - Resolver AGORA (1 dia)

**Implementar OpÃ§Ã£o 1:**

1. **Modificar cliente** (`components/Form.tsx`):

   ```typescript
   // Dividir arquivo em chunks pequenos
   const LINES_PER_REQUEST = 40;

   for (let i = 0; i < allSegments.length; i += LINES_PER_REQUEST) {
     const chunk = allSegments.slice(i, i + LINES_PER_REQUEST);

     const response = await fetch('/api/translate-chunk', {
       method: 'POST',
       body: JSON.stringify({ chunk, apiKey, filename }),
     });

     const result = await response.json();
     translatedSegments.push(...result.segments);

     // Atualizar progresso
     setProgress(...);
   }
   ```

2. **Criar novo endpoint** (`app/api/translate-chunk/route.ts`):

   ```typescript
   export async function POST(request: Request) {
   	const { chunk, apiKey, filename } = await request.json();

   	// Traduzir APENAS este chunk (rÃ¡pido!)
   	const translated = await translateChunk(chunk, apiKey);

   	// Retornar JSON (nÃ£o SSE)
   	return Response.json({
   		segments: translated,
   		processedCount: chunk.length,
   	});
   }
   ```

3. **Testar:**
   - Arquivo pequeno (10 linhas)
   - Arquivo mÃ©dio (100 linhas)
   - Arquivo grande (1000+ linhas)

4. **Deploy no Vercel**

**Tempo estimado:** 4-6 horas

---

### SPRINT 5+ - Quando Monetizar (futuro)

**Migrar para OpÃ§Ã£o 2 (Background Jobs):**

1. Setup Upstash Redis
2. Criar sistema de jobs
3. Implementar workers
4. Implementar polling no cliente
5. Migrar usuÃ¡rios gradualmente

**Tempo estimado:** 2-3 dias

---

## ğŸ“Š ComparaÃ§Ã£o Final

| CritÃ©rio                 | OpÃ§Ã£o 1: MÃºltiplas Reqs | OpÃ§Ã£o 2: Background Jobs | OpÃ§Ã£o 5: Migrar Render |
| ------------------------ | ----------------------- | ------------------------ | ---------------------- |
| **Tempo implementaÃ§Ã£o**  | 4-6 horas               | 2-3 dias                 | 2 horas                |
| **Complexidade**         | Baixa                   | Alta                     | BaixÃ­ssima             |
| **Custo adicional**      | $0                      | ~$10/mÃªs (Redis)         | ~$7/mÃªs (instÃ¢ncia)    |
| **Funciona no Vercel**   | âœ… Sim                  | âœ… Sim                   | âŒ NÃ£o (outro host)    |
| **Escalabilidade**       | âœ… Alta                 | âœ… Muito alta            | âš ï¸ MÃ©dia               |
| **Resolve timeout**      | âœ… Sim (100%)           | âœ… Sim (100%)            | âœ… Sim                 |
| **Arquitetura limpa**    | âœ… Sim                  | âœ… Sim                   | âŒ NÃ£o muda nada       |
| **Pronto para produÃ§Ã£o** | âœ… Sim                  | âœ… Sim                   | âš ï¸ TemporÃ¡rio          |

**Winner:** ğŸ† **OpÃ§Ã£o 1** para resolver AGORA, **OpÃ§Ã£o 2** para longo prazo.

---

## ğŸ¤” Perguntas Frequentes

### "Por que nÃ£o usar Vercel Edge Functions?"

- Edge Functions tambÃ©m tÃªm limite de tempo
- NÃ£o resolve o problema raiz (muito processamento em uma req)

### "Por que nÃ£o processar em paralelo?"

- Gemini tem rate limit (10 req/min no free tier)
- Processar em paralelo vai estourar rate limit
- Melhor fazer sequencial com controle

### "Por que nÃ£o usar outro modelo (GPT-4)?"

- Problema nÃ£o Ã© o modelo, Ã© a arquitetura
- Qualquer modelo vai ter o mesmo timeout

### "Por que nÃ£o cachear traduÃ§Ãµes?"

- Cache ajuda em reprocessamento, mas nÃ£o resolve primeiro processamento
- Vale implementar DEPOIS (Fase 2)

---

**PrÃ³xima aÃ§Ã£o:** Decidir entre OpÃ§Ã£o 1 (rÃ¡pida) ou OpÃ§Ã£o 2 (profissional) e comeÃ§ar implementaÃ§Ã£o.

---

**Ãšltima atualizaÃ§Ã£o:** 30 de dezembro de 2025  
**Autor:** GitHub Copilot + Tiago
